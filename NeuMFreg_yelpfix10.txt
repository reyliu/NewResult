liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001 --data yelpfix10
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelpfix10', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.4 s]. #user=13679, #item=12922, #train=68395, #cv=68395, #test=503353
Init: NDCG = 
[ 0.61491881  0.65622043  0.71092014  0.77589206  0.856227  ]
Iteration 0 [5.3 s]: loss = 15.6094 [7.5 s], NDCG = 
[ 0.64073568  0.67569975  0.72633339  0.78876095  0.86459274]
Iteration 1 [3.0 s]: loss = 14.1353 [7.4 s], NDCG = 
[ 0.65313564  0.68827605  0.73469099  0.79509106  0.86940174]
Iteration 2 [3.0 s]: loss = 9.6122 [7.5 s], NDCG = 
[ 0.65765304  0.69192272  0.73825942  0.79762267  0.87105312]
Iteration 3 [3.4 s]: loss = 3.3638 [7.9 s], NDCG = 
[ 0.67189659  0.70251996  0.74694676  0.80440271  0.87575075]
Iteration 4 [3.1 s]: loss = 1.4313 [7.8 s], NDCG = 
[ 0.69426489  0.72006371  0.76153673  0.81626736  0.8833458 ]
Iteration 5 [3.9 s]: loss = 1.1479 [7.9 s], NDCG = 
[ 0.70937087  0.7322905   0.77257148  0.82459554  0.88862926]
Iteration 6 [3.4 s]: loss = 1.0138 [7.5 s], NDCG = 
[ 0.7190128   0.74073314  0.77993828  0.83037158  0.89215623]
Iteration 7 [3.1 s]: loss = 0.9336 [7.5 s], NDCG = 
[ 0.72298341  0.74579993  0.78434164  0.83412624  0.89408294]
Iteration 8 [2.9 s]: loss = 0.8815 [7.6 s], NDCG = 
[ 0.72762043  0.74882641  0.78779151  0.83674099  0.8956625 ]
Iteration 9 [3.1 s]: loss = 0.8455 [7.6 s], NDCG = 
[ 0.72889283  0.75101798  0.78934504  0.83827554  0.89639911]
Iteration 10 [3.2 s]: loss = 0.8191 [7.6 s], NDCG = 
[ 0.73057642  0.75303308  0.79114646  0.83947238  0.89711827]
Iteration 11 [3.2 s]: loss = 0.7992 [7.6 s], NDCG = 
[ 0.73154073  0.75412285  0.79218513  0.84032091  0.8975209 ]
Iteration 12 [3.6 s]: loss = 0.7842 [7.6 s], NDCG = 
[ 0.73271243  0.75512973  0.79303681  0.84110752  0.89785975]
Iteration 13 [3.5 s]: loss = 0.7725 [7.8 s], NDCG = 
[ 0.73339523  0.7544151   0.7935259   0.84105865  0.89791046]
Iteration 14 [3.1 s]: loss = 0.7631 [7.8 s], NDCG = 
[ 0.73304303  0.75459465  0.79325023  0.84137309  0.89789679]
Test NDCG = 
[ 0.71802221  0.70994994  0.70723734  0.70757868  0.71028562  0.71537376
  0.72222768  0.7308381   0.74056266  0.75136906]


liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001 --dataset yelpfix10
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelpfix10', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.9 s]. #user=13679, #item=12922, #train=68395, #cv=68395, #test=503353
Init: NDCG = 
[ 0.61884863  0.66083703  0.71331167  0.77865632  0.85783614]
Iteration 0 [5.2 s]: loss = 15.3413 [8.0 s], NDCG = 
[ 0.64851429  0.68214459  0.73130555  0.79240053  0.86749466]
Iteration 1 [3.3 s]: loss = 11.9915 [8.1 s], NDCG = 
[ 0.65872134  0.69059245  0.73794568  0.79718844  0.87089566]
Iteration 2 [4.2 s]: loss = 4.4171 [8.4 s], NDCG = 
[ 0.67235934  0.70195691  0.74709151  0.80453541  0.87570494]
Iteration 3 [6.3 s]: loss = 1.4343 [8.4 s], NDCG = 
[ 0.70100315  0.72481685  0.76591054  0.81932361  0.88544452]
Iteration 4 [4.2 s]: loss = 1.1148 [8.0 s], NDCG = 
[ 0.71550947  0.73782087  0.77717896  0.82876144  0.8908578 ]
Iteration 5 [3.5 s]: loss = 0.9789 [8.0 s], NDCG = 
[ 0.72304867  0.74585213  0.78469613  0.83454741  0.89418509]
Iteration 6 [3.5 s]: loss = 0.9024 [8.1 s], NDCG = 
[ 0.72679155  0.74979315  0.78813283  0.83759194  0.89576514]
Iteration 7 [6.3 s]: loss = 0.8552 [8.5 s], NDCG = 
[ 0.72954091  0.75195278  0.79100119  0.83974693  0.89681564]
Iteration 8 [5.8 s]: loss = 0.8236 [8.3 s], NDCG = 
[ 0.73202104  0.75342345  0.79241757  0.84052141  0.89747861]
Iteration 9 [3.9 s]: loss = 0.8016 [8.1 s], NDCG = 
[ 0.73259892  0.753126    0.79295641  0.84073744  0.89754278]
Iteration 10 [3.4 s]: loss = 0.7855 [8.2 s], NDCG = 
[ 0.73256226  0.75384807  0.79303332  0.8410325   0.89770367]
Iteration 11 [3.7 s]: loss = 0.7735 [8.4 s], NDCG = 
[ 0.73176532  0.75406734  0.79332486  0.84123881  0.89768138]
Test NDCG = 
[ 0.71670224  0.70964329  0.70651393  0.70695902  0.7097856   0.71512248
  0.72188762  0.73036134  0.74022814  0.75121078]
liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001 --dataset yelpfix10
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelpfix10', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.8 s]. #user=13679, #item=12922, #train=68395, #cv=68395, #test=503353
Init: NDCG = 
[ 0.61821371  0.6597852   0.71159026  0.77699838  0.85712227]
Iteration 0 [6.8 s]: loss = 15.0837 [8.1 s], NDCG = 
[ 0.6466443   0.68110857  0.73055306  0.79127828  0.86698074]
Iteration 1 [3.7 s]: loss = 10.3592 [8.1 s], NDCG = 
[ 0.6583942   0.69274233  0.73884896  0.79758892  0.87132604]
Iteration 2 [3.7 s]: loss = 2.9204 [8.3 s], NDCG = 
[ 0.67971123  0.71049527  0.75379474  0.80934139  0.87885372]
Iteration 3 [5.9 s]: loss = 1.2733 [8.6 s], NDCG = 
[ 0.70759305  0.73023859  0.77151118  0.82453623  0.88800196]
Iteration 4 [6.3 s]: loss = 1.0496 [8.5 s], NDCG = 
[ 0.72087556  0.74315305  0.78259097  0.83257163  0.8931391 ]
Iteration 5 [3.7 s]: loss = 0.9397 [8.2 s], NDCG = 
[ 0.72773766  0.74944291  0.7881468   0.83692273  0.89579785]
Iteration 6 [3.8 s]: loss = 0.8757 [8.2 s], NDCG = 
[ 0.730909    0.75333924  0.79094653  0.83955138  0.89717591]
Iteration 7 [3.4 s]: loss = 0.8356 [8.6 s], NDCG = 
[ 0.73390362  0.75504571  0.79313343  0.84113267  0.89809953]
Iteration 8 [6.2 s]: loss = 0.8092 [8.6 s], NDCG = 
[ 0.73428403  0.75513111  0.79374693  0.84148415  0.89825034]
Iteration 9 [4.3 s]: loss = 0.7904 [8.1 s], NDCG = 
[ 0.7340359   0.75437917  0.79386268  0.84148665  0.89808389]
Test NDCG = 
[ 0.72158909  0.71172658  0.70835938  0.70863157  0.71130168  0.716393
  0.72270524  0.73118866  0.74141192  0.75223165]

