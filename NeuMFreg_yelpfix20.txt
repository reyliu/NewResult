liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001 --data yelpfix20
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelpfix20', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [3.8 s]. #user=7486, #item=12922, #train=112290, #cv=37430, #test=343504
Init: NDCG = 
[ 0.61726504  0.66065944  0.71353637  0.77787301  0.85849699]
Iteration 0 [6.4 s]: loss = 13.5936 [3.5 s], NDCG = 
[ 0.65555997  0.69173196  0.73707728  0.79789235  0.87138948]
Iteration 1 [4.8 s]: loss = 4.1237 [3.4 s], NDCG = 
[ 0.68350785  0.7134458   0.75636629  0.81209774  0.88079708]
Iteration 2 [4.7 s]: loss = 1.2062 [3.5 s], NDCG = 
[ 0.71974057  0.74420164  0.78223834  0.83399287  0.89390907]
Iteration 3 [4.4 s]: loss = 1.0436 [3.4 s], NDCG = 
[ 0.73391141  0.75740391  0.79439822  0.84356151  0.89952373]
Iteration 4 [4.5 s]: loss = 0.9786 [3.6 s], NDCG = 
[ 0.74065391  0.7641323   0.80145245  0.84781659  0.90236237]
Iteration 5 [5.2 s]: loss = 0.9447 [3.7 s], NDCG = 
[ 0.74494079  0.76757059  0.80491306  0.8497616   0.90391381]
Iteration 6 [4.8 s]: loss = 0.9249 [3.7 s], NDCG = 
[ 0.74483548  0.76907157  0.80597702  0.85057585  0.90427747]
Iteration 7 [5.0 s]: loss = 0.9128 [3.5 s], NDCG = 
[ 0.74702739  0.77034932  0.80670715  0.85111592  0.90477441]
Iteration 8 [4.6 s]: loss = 0.9050 [3.5 s], NDCG = 
[ 0.74802751  0.77053695  0.80715876  0.85157715  0.90502288]
Iteration 9 [4.9 s]: loss = 0.8997 [3.5 s], NDCG = 
[ 0.74770864  0.77017906  0.80740425  0.851799    0.90506375]
Iteration 10 [4.9 s]: loss = 0.8957 [3.6 s], NDCG = 
[ 0.74752018  0.77030401  0.80740623  0.85146936  0.90497244]
Test NDCG = 
[ 0.73266595  0.72402221  0.71941376  0.71937875  0.72127193  0.72519122
  0.73007593  0.73653391  0.74395597  0.75253322]
