liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.5 s]. #user=13679, #item=12922, #train=165069, #cv=158279, #test=316795
Init: NDCG = 
[ 0.5861202   0.60568638  0.63351009  0.66716275  0.70799304]
Iteration 0 [9.4 s]: loss = 12.3512 [13.2 s], NDCG = 
[ 0.63736194  0.65138461  0.67216453  0.70254718  0.73900661]
Iteration 1 [7.2 s]: loss = 2.1343 [12.9 s], NDCG = 
[ 0.69182039  0.6970158   0.71555565  0.74084876  0.77242339]
Iteration 2 [7.4 s]: loss = 1.1303 [12.9 s], NDCG = 
[ 0.7208938   0.72614863  0.74134082  0.76525613  0.79443633]
Iteration 3 [7.4 s]: loss = 0.9935 [13.1 s], NDCG = 
[ 0.73251119  0.7361121   0.75199154  0.7752332   0.80307535]
Iteration 4 [7.3 s]: loss = 0.9392 [13.0 s], NDCG = 
[ 0.73696158  0.74145434  0.75580324  0.77919431  0.80676891]
Iteration 5 [7.3 s]: loss = 0.9144 [13.2 s], NDCG = 
[ 0.74041333  0.74350653  0.75817596  0.78088074  0.80816034]
Iteration 6 [7.7 s]: loss = 0.9016 [12.9 s], NDCG = 
[ 0.74145212  0.74388928  0.75892454  0.7814321   0.80899072]
Iteration 7 [7.4 s]: loss = 0.8942 [12.9 s], NDCG = 
[ 0.74257369  0.74442171  0.75932084  0.78219506  0.80942954]
Iteration 8 [7.5 s]: loss = 0.8897 [12.5 s], NDCG = 
[ 0.74206525  0.74457723  0.75998579  0.7826047   0.80978238]
Iteration 9 [7.2 s]: loss = 0.8862 [12.8 s], NDCG = 
[ 0.7406766   0.74476583  0.75924083  0.78225387  0.80942814]
Test NDCG = 
[ 0.75360556  0.74127317  0.73818959  0.73957594  0.74451482  0.75209797
  0.76199968  0.77351037  0.78679125  0.80093973]

liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --lr 0.0001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.6 s]. #user=13679, #item=12922, #train=165069, #cv=158279, #test=316795
Init: NDCG = 
[ 0.5865277   0.60552578  0.63401389  0.66743634  0.70880569]
Iteration 0 [10.3 s]: loss = 11.5577 [14.1 s], NDCG = 
[ 0.64010314  0.653092    0.67497722  0.70424737  0.74088047]
Iteration 1 [8.9 s]: loss = 1.5701 [14.1 s], NDCG = 
[ 0.70699132  0.70983866  0.72684459  0.75058599  0.78160104]
Iteration 2 [8.6 s]: loss = 1.0733 [15.6 s], NDCG = 
[ 0.73032088  0.73116566  0.74709816  0.77053873  0.79890443]
Iteration 3 [8.0 s]: loss = 0.9700 [13.7 s], NDCG = 
[ 0.73374649  0.73775495  0.7536723   0.77729659  0.80476251]
Iteration 4 [7.9 s]: loss = 0.9295 [14.4 s], NDCG = 
[ 0.7378139   0.74178065  0.75689979  0.78041748  0.80766165]
Iteration 5 [9.1 s]: loss = 0.9109 [14.3 s], NDCG = 
[ 0.74202415  0.74350835  0.75906939  0.78168442  0.80892186]
Iteration 6 [9.1 s]: loss = 0.9012 [14.0 s], NDCG = 
[ 0.74285443  0.74549498  0.7603481   0.78276194  0.80988455]
Iteration 7 [8.1 s]: loss = 0.8954 [13.2 s], NDCG = 
[ 0.74184673  0.74639303  0.75991052  0.7828809   0.80982156]
Test NDCG = 
[ 0.75622947  0.74404245  0.73981811  0.73958858  0.74478439  0.75270418
  0.76230213  0.77389697  0.78708932  0.80128198]

liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --lr 0.0001Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.6 s]. #user=13679, #item=12922, #train=165069, #cv=158279, #test=316795
Init: NDCG = 
[ 0.58340488  0.59918337  0.62802282  0.66216024  0.70444818]
Iteration 0 [10.6 s]: loss = 13.2305 [14.9 s], NDCG = 
[ 0.63196225  0.64662241  0.66971366  0.69918398  0.73627504]
Iteration 1 [8.3 s]: loss = 3.0374 [14.5 s], NDCG = 
[ 0.67632662  0.68366088  0.70294415  0.72928929  0.76239016]
Iteration 2 [8.5 s]: loss = 1.2272 [14.3 s], NDCG = 
[ 0.71427255  0.71727505  0.73473676  0.75848033  0.78790292]
Iteration 3 [8.9 s]: loss = 1.0347 [16.0 s], NDCG = 
[ 0.72744228  0.73142654  0.747765    0.77078141  0.79871148]
Iteration 4 [8.4 s]: loss = 0.9593 [14.5 s], NDCG = 
[ 0.73331339  0.73760776  0.75384138  0.77664706  0.80423303]
Iteration 5 [9.3 s]: loss = 0.9238 [15.9 s], NDCG = 
[ 0.73861444  0.74294288  0.75716246  0.77997221  0.80718066]
Iteration 6 [10.2 s]: loss = 0.9059 [13.8 s], NDCG = 
[ 0.74172257  0.74471892  0.75948831  0.78186829  0.80861671]
Iteration 7 [8.6 s]: loss = 0.8962 [14.8 s], NDCG = 
[ 0.74201454  0.7444819   0.75876233  0.78192523  0.80906034]
Iteration 8 [9.1 s]: loss = 0.8903 [14.1 s], NDCG = 
[ 0.74306604  0.74521169  0.76016807  0.7827828   0.80983231]
Iteration 9 [9.6 s]: loss = 0.8864 [14.8 s], NDCG = 
[ 0.7423256   0.74484916  0.75969578  0.78282504  0.80975593]
Test NDCG = 
[ 0.75463721  0.74245223  0.73906585  0.73979198  0.74454985  0.75217838
  0.7622845   0.77360836  0.78690314  0.80135404]

liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --lr 0.0001Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.6 s]. #user=13679, #item=12922, #train=165069, #cv=158279, #test=316795
Init: NDCG = 
[ 0.58425829  0.60376317  0.63178187  0.66624433  0.7074189 ]
Iteration 0 [9.9 s]: loss = 10.3011 [14.2 s], NDCG = 
[ 0.64500356  0.65667833  0.67915814  0.70771243  0.74488025]
Iteration 1 [9.0 s]: loss = 1.3528 [16.4 s], NDCG = 
[ 0.7132255   0.71624918  0.73322127  0.75732217  0.78772958]
Iteration 2 [7.4 s]: loss = 1.0390 [12.6 s], NDCG = 
[ 0.73203573  0.73371551  0.75017849  0.77341751  0.80161467]
Iteration 3 [7.2 s]: loss = 0.9565 [13.0 s], NDCG = 
[ 0.73596156  0.74023823  0.75571009  0.7788618   0.80635161]
Iteration 4 [7.4 s]: loss = 0.9243 [12.7 s], NDCG = 
[ 0.7407699   0.74511601  0.75920737  0.78157225  0.80878841]
Iteration 5 [7.5 s]: loss = 0.9091 [13.3 s], NDCG = 
[ 0.73964563  0.74491322  0.7596719   0.78213397  0.80928741]
Iteration 6 [7.8 s]: loss = 0.9008 [13.2 s], NDCG = 
[ 0.74210231  0.74578335  0.76009927  0.78298227  0.80977945]
Iteration 7 [8.3 s]: loss = 0.8956 [13.5 s], NDCG = 
[ 0.74258518  0.7452599   0.76010723  0.78266313  0.80940721]
Test NDCG = 
[ 0.75664295  0.74277749  0.73892021  0.7403218   0.74475214  0.75252311
  0.76243911  0.77413322  0.78732603  0.80144526]

liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --lr 0.0001Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.4 s]. #user=13679, #item=12922, #train=165069, #cv=158279, #test=316795
Init: NDCG = 
[ 0.58232554  0.60117472  0.62908557  0.66393091  0.70526891]
Iteration 0 [11.7 s]: loss = 12.1843 [13.9 s], NDCG = 
[ 0.63683963  0.65131119  0.67394815  0.70284069  0.73948722]
Iteration 1 [9.0 s]: loss = 1.9283 [14.4 s], NDCG = 
[ 0.6961485   0.7012896   0.71846124  0.74362631  0.77571104]
Iteration 2 [8.5 s]: loss = 1.1175 [14.9 s], NDCG = 
[ 0.72470643  0.72767426  0.74443374  0.76767642  0.79581426]
Iteration 3 [9.3 s]: loss = 0.9878 [15.5 s], NDCG = 
[ 0.73294461  0.73680229  0.75282394  0.7762143   0.80313205]
Iteration 4 [9.4 s]: loss = 0.9373 [15.5 s], NDCG = 
[ 0.73778057  0.74141794  0.75662733  0.77949646  0.80698951]
Iteration 5 [8.6 s]: loss = 0.9142 [14.4 s], NDCG = 
[ 0.74024412  0.74400918  0.75852528  0.78130018  0.80834595]
Iteration 6 [8.3 s]: loss = 0.9024 [14.5 s], NDCG = 
[ 0.74074658  0.7463301   0.76005408  0.78231672  0.80941122]
Iteration 7 [9.1 s]: loss = 0.8956 [15.2 s], NDCG = 
[ 0.74323354  0.74626511  0.76022833  0.78324575  0.81008132]
Iteration 8 [8.6 s]: loss = 0.8913 [14.5 s], NDCG = 
[ 0.74189438  0.74486859  0.75927311  0.7824747   0.80969835]
Test NDCG = 
[ 0.7550903   0.74260505  0.73869927  0.73954592  0.74466005  0.75257307
  0.76198308  0.77357924  0.78693519  0.80130026]

2. MLP change
liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --lr 0.0001 --layers [128,32,8] --reg_layers [0,0,0]
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[128,32,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0]', verbose=1) 
Load data done [4.7 s]. #user=13679, #item=12922, #train=165069, #cv=158279, #test=316795
Init: NDCG = 
[ 0.58243473  0.60102905  0.62811625  0.66296814  0.70544086]
Iteration 0 [15.3 s]: loss = 12.6540 [13.7 s], NDCG = 
[ 0.63361364  0.64880063  0.67078973  0.70087739  0.73710696]
Iteration 1 [13.5 s]: loss = 2.7967 [13.7 s], NDCG = 
[ 0.68577948  0.6905963   0.70772527  0.73300036  0.76609849]
Iteration 2 [13.4 s]: loss = 1.2361 [13.8 s], NDCG = 
[ 0.71768065  0.72215282  0.73784177  0.76101171  0.79035099]
Iteration 3 [13.5 s]: loss = 1.0268 [13.9 s], NDCG = 
[ 0.72968825  0.73407474  0.74946455  0.77236509  0.80055252]
Iteration 4 [13.4 s]: loss = 0.9564 [13.7 s], NDCG = 
[ 0.73752413  0.74001023  0.75491415  0.77831259  0.80573776]
Iteration 5 [13.6 s]: loss = 0.9283 [13.7 s], NDCG = 
[ 0.74109039  0.74324206  0.75776798  0.78110977  0.80802366]
Iteration 6 [13.6 s]: loss = 0.9150 [13.7 s], NDCG = 
[ 0.74172356  0.74499723  0.75946377  0.7817057   0.80888891]
Iteration 7 [13.4 s]: loss = 0.9080 [13.6 s], NDCG = 
[ 0.74260733  0.74497095  0.75941329  0.78237791  0.8094227 ]
Iteration 8 [13.6 s]: loss = 0.9034 [13.7 s], NDCG = 
[ 0.74137782  0.74375596  0.75951361  0.78199973  0.80931187]
Test NDCG = 
[ 0.75375901  0.74226825  0.73805388  0.73920826  0.74435352  0.75235189
  0.76190391  0.77345221  0.78665445  0.80082553]


