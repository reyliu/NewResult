liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001 --data amazonfix10
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='amazonfix10', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [11.6 s]. #user=28881, #item=30133, #train=144405, #cv=144405, #test=1277634
Init: NDCG = 
[ 0.72225415  0.74772548  0.78510768  0.83368269  0.89844837]
Iteration 0 [14.5 s]: loss = 16.9601 [25.2 s], NDCG = 
[ 0.74608398  0.76693376  0.79971415  0.8447349   0.90578439]
Iteration 1 [13.7 s]: loss = 6.1185 [25.4 s], NDCG = 
[ 0.75706217  0.77402742  0.8056592   0.84988187  0.90899881]
Iteration 2 [14.2 s]: loss = 1.0057 [25.2 s], NDCG = 
[ 0.77680771  0.79066673  0.81857694  0.86005638  0.91558332]
Iteration 3 [13.5 s]: loss = 0.7150 [24.2 s], NDCG = 
[ 0.78665212  0.79944524  0.82656494  0.86602049  0.91918984]
Iteration 4 [13.1 s]: loss = 0.6143 [24.6 s], NDCG = 
[ 0.79168869  0.80377926  0.83015659  0.86913457  0.9209616 ]
Iteration 5 [13.5 s]: loss = 0.5694 [25.4 s], NDCG = 
[ 0.7917156   0.805425    0.83166706  0.87024143  0.92143594]
Iteration 6 [13.5 s]: loss = 0.5453 [25.6 s], NDCG = 
[ 0.79159761  0.80616858  0.83260833  0.87061259  0.92164592]
Iteration 7 [13.5 s]: loss = 0.5304 [25.2 s], NDCG = 
[ 0.79168801  0.80625668  0.83288446  0.8709363   0.92173971]
Iteration 8 [13.4 s]: loss = 0.5201 [25.5 s], NDCG = 
[ 0.79080025  0.8063046   0.83272748  0.87103084  0.92165168]
Test NDCG = 
[ 0.78275127  0.78266937  0.78206395  0.78222387  0.78340347  0.78638935
  0.79027623  0.79529627  0.80127344  0.80859673]

