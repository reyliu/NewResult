liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001 --data moviefix50
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='moviefix50', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [68.5 s]. #user=38596, #item=8940, #train=1736820, #cv=192980, #test=6952446
Init: NDCG = 
[ 0.60505451  0.65901699  0.71857136  0.78358209  0.85758708]
Iteration 0 [191.0 s]: loss = 1.7914 [46.8 s], NDCG = 
[ 0.7633791   0.79985144  0.83949659  0.87864102  0.91715836]
Iteration 1 [306.8 s]: loss = 0.7809 [43.1 s], NDCG = 
[ 0.76377705  0.79982539  0.84063793  0.87974411  0.91744348]
Iteration 2 [292.3 s]: loss = 0.7667 [42.9 s], NDCG = 
[ 0.7636732   0.80115285  0.84144903  0.88042043  0.91775013]
Iteration 3 [292.1 s]: loss = 0.7471 [46.1 s], NDCG = 
[ 0.76522767  0.80252983  0.8428729   0.88152979  0.91839299]
Iteration 4 [289.2 s]: loss = 0.7231 [44.6 s], NDCG = 
[ 0.76816368  0.80538521  0.84462213  0.88300609  0.91943809]
Iteration 5 [166.6 s]: loss = 0.7093 [41.2 s], NDCG = 
[ 0.76980799  0.80683289  0.84630905  0.88385718  0.92014389]
Iteration 6 [160.5 s]: loss = 0.6988 [38.2 s], NDCG = 
[ 0.77333766  0.80898222  0.84760243  0.88487722  0.92106292]
Iteration 7 [158.1 s]: loss = 0.6875 [38.0 s], NDCG = 
[ 0.77443652  0.80976993  0.848026    0.88524342  0.92139143]
Iteration 8 [153.0 s]: loss = 0.6746 [37.2 s], NDCG = 
[ 0.77616629  0.81035425  0.84848979  0.88592008  0.92180496]
Iteration 9 [153.9 s]: loss = 0.6616 [40.1 s], NDCG = 
[ 0.77447619  0.80970646  0.84804845  0.88527716  0.9213957 ]
Test NDCG = 
[ 0.73797394  0.72902681  0.72527218  0.72326465  0.72294468  0.72352064
  0.72504921  0.72680589  0.72900338  0.73140378]

liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001 --data moviefix50
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='moviefix50', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [70.5 s]. #user=38596, #item=8940, #train=1736820, #cv=192980, #test=6952446
Init: NDCG = 
[ 0.59994407  0.65607978  0.71625921  0.7815217   0.85619109]
Iteration 0 [300.7 s]: loss = 1.6046 [41.4 s], NDCG = 
[ 0.76210315  0.79906974  0.83931741  0.87889176  0.91690047]
Iteration 1 [271.7 s]: loss = 0.7805 [36.7 s], NDCG = 
[ 0.76284552  0.79983297  0.84025847  0.87959666  0.91724012]
Iteration 2 [156.9 s]: loss = 0.7678 [39.5 s], NDCG = 
[ 0.76403212  0.80061209  0.84129578  0.88039034  0.91771724]
Iteration 3 [186.0 s]: loss = 0.7517 [52.3 s], NDCG = 
[ 0.76659251  0.80284416  0.84248086  0.88157045  0.91853708]
Iteration 4 [326.5 s]: loss = 0.7253 [51.7 s], NDCG = 
[ 0.76800909  0.80483008  0.84393095  0.88277784  0.91923533]
Iteration 5 [321.4 s]: loss = 0.7078 [48.0 s], NDCG = 
[ 0.77207756  0.80812141  0.84687141  0.88442633  0.92063886]
Iteration 6 [312.4 s]: loss = 0.6927 [52.4 s], NDCG = 
[ 0.7751527   0.81011158  0.84814026  0.88574076  0.92155876]
Iteration 7 [297.1 s]: loss = 0.6750 [44.9 s], NDCG = 
[ 0.77649705  0.81056568  0.84843733  0.88588182  0.92185199]
Iteration 8 [156.9 s]: loss = 0.6576 [38.8 s], NDCG = 
[ 0.77455175  0.8097765   0.84784266  0.88506738  0.92135062]
Test NDCG = 
[ 0.73994638  0.73084189  0.72597116  0.72351117  0.72311648  0.72401019
  0.72504326  0.72671016  0.72880296  0.73128344]


liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --lr 0.0001 --data moviefix50
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='moviefix50', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [67.8 s]. #user=38596, #item=8940, #train=1736820, #cv=192980, #test=6952446
Init: NDCG = 
[ 0.59849539  0.6520414   0.71235329  0.77831697  0.85478503]
Iteration 0 [294.4 s]: loss = 2.2269 [44.6 s], NDCG = 
[ 0.7608862   0.79743041  0.83807696  0.87724838  0.91625981]
Iteration 1 [314.2 s]: loss = 0.7844 [45.6 s], NDCG = 
[ 0.7629371   0.79999848  0.8404778   0.87961324  0.91730931]
Iteration 2 [316.2 s]: loss = 0.7697 [48.3 s], NDCG = 
[ 0.76316468  0.80038185  0.84094227  0.87993931  0.91748133]
Iteration 3 [207.0 s]: loss = 0.7614 [40.0 s], NDCG = 
[ 0.76440713  0.80159234  0.84166668  0.88078926  0.91796653]
Iteration 4 [162.9 s]: loss = 0.7487 [40.8 s], NDCG = 
[ 0.76552086  0.80239192  0.84288616  0.88147921  0.91840801]
Iteration 5 [233.8 s]: loss = 0.7261 [46.6 s], NDCG = 
[ 0.7696985   0.80615407  0.8450627   0.8834096   0.91981629]
Iteration 6 [303.7 s]: loss = 0.7105 [44.3 s], NDCG = 
[ 0.77158408  0.80766671  0.84669609  0.88426166  0.92050348]
Iteration 7 [296.8 s]: loss = 0.7003 [43.4 s], NDCG = 
[ 0.77291595  0.80918273  0.84753961  0.88498559  0.92101307]
Iteration 8 [312.2 s]: loss = 0.6911 [61.0 s], NDCG = 
[ 0.77580274  0.8102358   0.84856487  0.88583178  0.92170549]
Iteration 9 [225.1 s]: loss = 0.6803 [42.5 s], NDCG = 
[ 0.77511002  0.81016062  0.84835214  0.8857476   0.92158457]
Test NDCG = 
[ 0.7391168   0.73061689  0.72684265  0.72534221  0.72511455  0.7257449
  0.72714285  0.72888717  0.73111592  0.73362992]


